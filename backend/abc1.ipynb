{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess a video\n",
    "def load_video(video_path, frames_per_video=10, img_height=64, img_width=64):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while len(frames) < frames_per_video:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (img_width, img_height))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'Labels', 'Normal', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing subfolders with videos\n",
    "data_dir = 'DCSASS Dataset'\n",
    "class_names = os.listdir(data_dir)\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_height, img_width = 64, 64\n",
    "frames_per_video = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load videos and labels\n",
    "videos = []\n",
    "labels = []\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for root, _, files in os.walk(class_dir):\n",
    "        for video_file in files:\n",
    "            video_path = os.path.join(root, video_file)\n",
    "            frames = load_video(video_path, frames_per_video, img_height, img_width)\n",
    "            if len(frames) >= frames_per_video:\n",
    "                videos.append(frames[:frames_per_video])\n",
    "                labels.append(class_names.index(class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert videos and labels to numpy arrays\n",
    "videos = np.array(videos)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the videos array to match the expected input shape for a Conv3D model\n",
    "videos = videos.reshape(-1, frames_per_video, img_height, img_width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(frames_per_video, img_height, img_width, 3)),\n",
    "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n",
    "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "524/524 [==============================] - 544s 1s/step - loss: 2.7027 - accuracy: 0.4788\n",
      "Epoch 2/20\n",
      "524/524 [==============================] - 543s 1s/step - loss: 0.6240 - accuracy: 0.8172\n",
      "Epoch 3/20\n",
      "524/524 [==============================] - 578s 1s/step - loss: 0.3850 - accuracy: 0.8869\n",
      "Epoch 4/20\n",
      "524/524 [==============================] - 629s 1s/step - loss: 0.3083 - accuracy: 0.9075\n",
      "Epoch 5/20\n",
      "524/524 [==============================] - 620s 1s/step - loss: 0.2546 - accuracy: 0.9247\n",
      "Epoch 6/20\n",
      "524/524 [==============================] - 630s 1s/step - loss: 0.2507 - accuracy: 0.9241\n",
      "Epoch 7/20\n",
      "524/524 [==============================] - 627s 1s/step - loss: 0.2544 - accuracy: 0.9272\n",
      "Epoch 8/20\n",
      "524/524 [==============================] - 624s 1s/step - loss: 0.2071 - accuracy: 0.9417\n",
      "Epoch 9/20\n",
      "524/524 [==============================] - 597s 1s/step - loss: 0.2062 - accuracy: 0.9404\n",
      "Epoch 10/20\n",
      "524/524 [==============================] - 550s 1s/step - loss: 0.1965 - accuracy: 0.9446\n",
      "Epoch 11/20\n",
      "524/524 [==============================] - 561s 1s/step - loss: 0.1745 - accuracy: 0.9490\n",
      "Epoch 12/20\n",
      "524/524 [==============================] - 544s 1s/step - loss: 0.1883 - accuracy: 0.9448\n",
      "Epoch 13/20\n",
      "524/524 [==============================] - 542s 1s/step - loss: 0.2076 - accuracy: 0.9495\n",
      "Epoch 14/20\n",
      "524/524 [==============================] - 534s 1s/step - loss: 0.2233 - accuracy: 0.9434\n",
      "Epoch 15/20\n",
      "524/524 [==============================] - 565s 1s/step - loss: 0.1793 - accuracy: 0.9515\n",
      "Epoch 16/20\n",
      "524/524 [==============================] - 694s 1s/step - loss: 0.1689 - accuracy: 0.9546\n",
      "Epoch 17/20\n",
      "524/524 [==============================] - 619s 1s/step - loss: 0.1743 - accuracy: 0.9536\n",
      "Epoch 18/20\n",
      "524/524 [==============================] - 621s 1s/step - loss: 0.1538 - accuracy: 0.9600\n",
      "Epoch 19/20\n",
      "524/524 [==============================] - 622s 1s/step - loss: 0.1462 - accuracy: 0.9613\n",
      "Epoch 20/20\n",
      "524/524 [==============================] - 624s 1s/step - loss: 0.1425 - accuracy: 0.9621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1813eeffd00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the entire dataset\n",
    "model.fit(videos, labels, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sujal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('video_classification_model2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# predictions = model.predict(\"DCSASS Dataset/Explosion/Explosion004_x264.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524/524 [==============================] - 252s 471ms/step - loss: 0.0218 - accuracy: 0.9928\n",
      "Accuracy on the entire dataset: 0.9928315281867981\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the loaded model on the same dataset\n",
    "loss, accuracy = model.evaluate(videos, labels)\n",
    "print(\"Accuracy on the entire dataset:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sujal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('video_classification_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "\n",
    "# # Function to load and preprocess a single test video\n",
    "# def load_test_video(video_path, frames_per_video=10, img_height=64, img_width=64):\n",
    "#     frames = []\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     while len(frames) < frames_per_video:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         frame = cv2.resize(frame, (img_width, img_height))\n",
    "#         frames.append(frame)\n",
    "#     cap.release()\n",
    "#     return np.array(frames)\n",
    "\n",
    "# # Function to predict the class of a test video\n",
    "# def predict_video_class(video_path, model, class_names, frames_per_video=10):\n",
    "#     test_video = load_test_video(video_path, frames_per_video)\n",
    "#     test_video = test_video.reshape(-1, frames_per_video, img_height, img_width, 3)\n",
    "#     predictions = model.predict(test_video)\n",
    "#     predicted_class_index = np.argmax(predictions)\n",
    "#     predicted_class = class_names[predicted_class_index]\n",
    "#     return predicted_class\n",
    "\n",
    "# # Path to the directory containing subfolders with videos\n",
    "# main_folder = 'DCSASS Dataset'\n",
    "# class_names = os.listdir(main_folder)\n",
    "# num_classes = len(class_names)\n",
    "\n",
    "# # Load the saved model\n",
    "# loaded_model = keras.models.load_model('video_classification_model2.h5')\n",
    "\n",
    "# # Define the class names\n",
    "# class_names = ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'Labels', 'Normal', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']  # Modify this based on your actual class names\n",
    "\n",
    "# # Parameters for the test videos\n",
    "# img_height, img_width = 64, 64\n",
    "# frames_per_video = 10\n",
    "\n",
    "# # Function to process videos in a folder and its subfolders\n",
    "# def process_videos_in_folder(folder_path):\n",
    "#     for root, _, files in os.walk(folder_path):\n",
    "#         for file in files:\n",
    "#             video_path = os.path.join(root, file)\n",
    "#             if video_path.endswith(('.mp4', '.avi', '.mpg', '.mov', '.flv', '.wmv')):\n",
    "#                 predicted_class = predict_video_class(video_path, loaded_model, class_names, frames_per_video)\n",
    "#                 print(\"Video:\", video_path)\n",
    "#                 print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "# # Process videos in the main folder and its subfolders\n",
    "# process_videos_in_folder(main_folder)\n",
    "\n",
    "# # Additionally, if you want to process videos in the folder with only videos\n",
    "# videos_folder = 'DCSASS Dataset/Shooting/Shooting048_x264.mp4/Shooting048_x264_1.mp4'\n",
    "# process_videos_in_folder(videos_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Function to load and preprocess a single test video\n",
    "def load_test_video(video_path, frames_per_video=10, img_height=64, img_width=64):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while len(frames) < frames_per_video:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (img_width, img_height))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# Function to predict the class of a test video\n",
    "def predict_video_class(video_path, model, class_names, frames_per_video=10):\n",
    "    test_video = load_test_video(video_path, frames_per_video)\n",
    "    test_video = test_video.reshape(-1, frames_per_video, img_height, img_width, 3)\n",
    "    predictions = model.predict(test_video)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class = class_names[predicted_class_index]\n",
    "    return predicted_class\n",
    "\n",
    "loaded_model = keras.models.load_model('video_classification_model2.h5')\n",
    "\n",
    "class_names = ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'Labels', 'Normal', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']  # Modify this based on your actual class names\n",
    "\n",
    "img_height, img_width = 64, 64\n",
    "frames_per_video = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend/DCSASS Dataset/Arrest/Arrest001_x264.mp4/Arrest001_x264_0.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_video_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print the predicted class\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted class:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_class)\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mpredict_video_class\u001b[1;34m(video_path, model, class_names, frames_per_video)\u001b[0m\n\u001b[0;32m     21\u001b[0m test_video \u001b[38;5;241m=\u001b[39m load_test_video(video_path, frames_per_video)\n\u001b[0;32m     22\u001b[0m test_video \u001b[38;5;241m=\u001b[39m test_video\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, frames_per_video, img_height, img_width, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_video\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m predicted_class_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions)\n\u001b[0;32m     25\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m class_names[predicted_class_index]\n",
      "File \u001b[1;32mc:\\Users\\Abhishek Thomba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Abhishek Thomba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\progbar.py:119\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    116\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     numdigits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog10\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    120\u001b[0m     bar \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(numdigits) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m (current, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m    121\u001b[0m     bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\x1b\u001b[39;00m\u001b[38;5;124m[1m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\x1b\u001b[39;00m\u001b[38;5;124m[0m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "test_video_path = 'backend/DCSASS Dataset/Arrest/Arrest001_x264.mp4/Arrest001_x264_0.mp4'\n",
    "\n",
    "predicted_class = predict_video_class(test_video_path, loaded_model, class_names)\n",
    "\n",
    "# Print the predicted class\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
